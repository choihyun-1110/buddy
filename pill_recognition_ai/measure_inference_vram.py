"""
ì¶”ë¡  ì‹œ VRAM ì‚¬ìš©ëŸ‰ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸
"""
import torch
from ultralytics import YOLO
from pathlib import Path
from utils.gpu_optimizer import GPUOptimizer
import gc

def measure_inference_vram(model_path: str, image_paths: list):
    """ì¶”ë¡  ì‹œ VRAM ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
    optimizer = GPUOptimizer()
    
    print("\n" + "="*60)
    print("ğŸ“Š ì¶”ë¡  ì „ ë©”ëª¨ë¦¬ ìƒíƒœ")
    print("="*60)
    optimizer.print_memory_status()
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    optimizer.clear_memory()
    
    print("\n" + "="*60)
    print("ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë“œ ì „ ë©”ëª¨ë¦¬
    before_load = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
    
    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(model_path)
    
    # ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    after_load = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0
    model_load_vram = after_load - before_load
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"   ëª¨ë¸ ë¡œë“œë¡œ ì¸í•œ VRAM ì¦ê°€: {model_load_vram:.2f}GB")
    
    print("\n" + "="*60)
    print("ğŸ” ì¶”ë¡  ì¤‘ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§")
    print("="*60)
    
    max_vram_used = 0
    inference_vram_usage = []
    
    for idx, img_path in enumerate(image_paths, 1):
        img_path = Path(img_path)
        if not img_path.exists():
            print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {img_path}")
            continue
        
        # ì¶”ë¡  ì „ ë©”ëª¨ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            before_inference = torch.cuda.memory_allocated() / 1024**3
        else:
            before_inference = 0
        
        print(f"\nğŸ“¸ ì´ë¯¸ì§€ {idx}: {img_path.name}")
        print(f"   ì¶”ë¡  ì „ VRAM: {before_inference:.2f}GB")
        
        # ì¶”ë¡  ì‹¤í–‰
        results = model.predict(str(img_path), imgsz=640, conf=0.25, verbose=False)
        
        # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            after_inference = torch.cuda.memory_allocated() / 1024**3
            peak_memory = torch.cuda.max_memory_allocated() / 1024**3
        else:
            after_inference = 0
            peak_memory = 0
        
        inference_vram = after_inference - before_inference
        inference_vram_usage.append(inference_vram)
        max_vram_used = max(max_vram_used, peak_memory)
        
        print(f"   ì¶”ë¡  í›„ VRAM: {after_inference:.2f}GB")
        print(f"   ì¶”ë¡ ìœ¼ë¡œ ì¸í•œ VRAM ì¦ê°€: {inference_vram:.2f}GB")
        print(f"   í”¼í¬ VRAM: {peak_memory:.2f}GB")
        
        # ê°ì§€ëœ ê°ì²´ ìˆ˜
        if results and len(results) > 0:
            detections = len(results[0].boxes) if results[0].boxes is not None else 0
            print(f"   ê°ì§€ëœ ì•Œì•½ ìˆ˜: {detections}ê°œ")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ìœ„í•´)
        del results
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½")
    print("="*60)
    
    if torch.cuda.is_available():
        final_vram = torch.cuda.memory_allocated() / 1024**3
        final_cached = torch.cuda.memory_reserved() / 1024**3
        
        print(f"ëª¨ë¸ ë¡œë“œ VRAM: {model_load_vram:.2f}GB")
        if inference_vram_usage:
            avg_inference_vram = sum(inference_vram_usage) / len(inference_vram_usage)
            print(f"í‰ê·  ì¶”ë¡  VRAM ì¦ê°€: {avg_inference_vram:.2f}GB")
            print(f"ìµœëŒ€ ì¶”ë¡  VRAM (í”¼í¬): {max_vram_used:.2f}GB")
        print(f"í˜„ì¬ í• ë‹¹ëœ VRAM: {final_vram:.2f}GB")
        print(f"í˜„ì¬ ìºì‹œëœ VRAM: {final_cached:.2f}GB")
        print(f"ì´ VRAM ì‚¬ìš©ë¥ : {(final_cached / optimizer.gpu_memory_total * 100):.1f}%")
        
        # ê¶Œì¥ ì‚¬í•­
        print("\nğŸ’¡ ê¶Œì¥ ì‚¬í•­:")
        if final_cached > optimizer.gpu_memory_total * 0.8:
            print("   âš ï¸ VRAM ì‚¬ìš©ë¥ ì´ 80% ì´ìƒì…ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.")
        elif final_cached > optimizer.gpu_memory_total * 0.6:
            print("   âœ… VRAM ì‚¬ìš©ë¥ ì´ ì ì ˆí•©ë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print("   âœ… VRAM ì‚¬ìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    optimizer.clear_memory()
    
    return {
        'model_load_vram': model_load_vram,
        'inference_vram_usage': inference_vram_usage,
        'max_inference_vram': max_vram_used,
        'final_vram': final_vram if torch.cuda.is_available() else 0,
        'final_cached': final_cached if torch.cuda.is_available() else 0
    }


if __name__ == "__main__":
    # ì¸¡ì •í•  ëª¨ë¸ê³¼ ì´ë¯¸ì§€ ê²½ë¡œ
    model_path = "results/training_20251031_201041/best_model.pt"
    image_paths = [
        "../real_image0.jpeg",
        "../real_image1.jpeg"
    ]
    
    # VRAM ì‚¬ìš©ëŸ‰ ì¸¡ì •
    results = measure_inference_vram(model_path, image_paths)
    
    print("\nâœ… VRAM ì¸¡ì • ì™„ë£Œ!")

