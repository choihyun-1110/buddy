"""
RTX 3060Ti 8GB VRAM ìµœì í™” ëª¨ë“ˆ
GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°
"""

import os
import torch
import gc
import psutil
import GPUtil
from typing import Dict, List, Optional
import warnings


class GPUOptimizer:
    """RTX 3060Ti 8GB VRAM ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.gpu_memory_total = 0
        self.gpu_memory_used = 0
        
        if torch.cuda.is_available():
            self.gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            print(f"ğŸ® GPU ê°ì§€: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ ì´ VRAM: {self.gpu_memory_total:.1f}GB")
        
        # ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
        self._setup_memory_optimization()
    
    def _setup_memory_optimization(self):
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •"""
        if torch.cuda.is_available():
            # CUDA ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •
            torch.cuda.empty_cache()
            
            # ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš© (ë‹¨ê³„ì  í• ë‹¹)
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
            
            # ë©”ëª¨ë¦¬ í’€ë§ í™œì„±í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            
            print("âœ… GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ")
    
    def get_memory_info(self) -> Dict:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        memory_info = {
            'cpu_memory': {
                'total': psutil.virtual_memory().total / 1024**3,
                'used': psutil.virtual_memory().used / 1024**3,
                'available': psutil.virtual_memory().available / 1024**3,
                'percent': psutil.virtual_memory().percent
            }
        }
        
        if torch.cuda.is_available():
            memory_info['gpu_memory'] = {
                'total': self.gpu_memory_total,
                'allocated': torch.cuda.memory_allocated() / 1024**3,
                'cached': torch.cuda.memory_reserved() / 1024**3,
                'free': (self.gpu_memory_total * 1024**3 - torch.cuda.memory_reserved()) / 1024**3
            }
            
            # GPU ì˜¨ë„ ë° ì‚¬ìš©ë¥  ì •ë³´
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    memory_info['gpu_memory']['temperature'] = gpu.temperature
                    memory_info['gpu_memory']['load'] = gpu.load * 100
            except:
                pass
        
        return memory_info
    
    def print_memory_status(self):
        """ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
        memory_info = self.get_memory_info()
        
        print("\nğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print(f"   CPU: {memory_info['cpu_memory']['used']:.1f}GB / {memory_info['cpu_memory']['total']:.1f}GB ({memory_info['cpu_memory']['percent']:.1f}%)")
        
        if 'gpu_memory' in memory_info:
            gpu_mem = memory_info['gpu_memory']
            print(f"   GPU: {gpu_mem['allocated']:.1f}GB / {gpu_mem['total']:.1f}GB (ì‚¬ìš©ë¥ : {gpu_mem['allocated']/gpu_mem['total']*100:.1f}%)")
            print(f"   GPU ìºì‹œ: {gpu_mem['cached']:.1f}GB")
            print(f"   GPU ì—¬ìœ : {gpu_mem['free']:.1f}GB")
            
            if 'temperature' in gpu_mem:
                print(f"   GPU ì˜¨ë„: {gpu_mem['temperature']}Â°C")
            if 'load' in gpu_mem:
                print(f"   GPU ì‚¬ìš©ë¥ : {gpu_mem['load']:.1f}%")
    
    def clear_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    def get_optimal_batch_size(self, model, input_size: tuple = (3, 640, 640), 
                              max_batch_size: int = 16, safety_margin: float = 0.8) -> int:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ìë™ íƒì§€"""
        if not torch.cuda.is_available():
            return 1
        
        print("ğŸ” ìµœì  ë°°ì¹˜ í¬ê¸° íƒì§€ ì¤‘...")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        self.clear_memory()
        
        optimal_batch_size = 1
        
        for batch_size in range(1, max_batch_size + 1):
            try:
                # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
                test_input = torch.randn(batch_size, *input_size).to(self.device)
                
                # ëª¨ë¸ì„ GPUë¡œ ì´ë™
                model = model.to(self.device)
                
                # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
                with torch.no_grad():
                    _ = model(test_input)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_ratio = memory_used / self.gpu_memory_total
                
                if memory_ratio < safety_margin:
                    optimal_batch_size = batch_size
                    print(f"   ë°°ì¹˜ í¬ê¸° {batch_size}: ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_ratio*100:.1f}% âœ…")
                else:
                    print(f"   ë°°ì¹˜ í¬ê¸° {batch_size}: ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_ratio*100:.1f}% âŒ (ì´ˆê³¼)")
                    break
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del test_input
                self.clear_memory()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"   ë°°ì¹˜ í¬ê¸° {batch_size}: OOM ì—ëŸ¬ âŒ")
                    break
                else:
                    raise e
        
        print(f"ğŸ¯ ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}")
        return optimal_batch_size
    
    def optimize_model_for_inference(self, model):
        """ì¶”ë¡ ìš© ëª¨ë¸ ìµœì í™”"""
        if torch.cuda.is_available():
            # ëª¨ë¸ì„ GPUë¡œ ì´ë™
            model = model.to(self.device)
            
            # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            model.eval()
            
            # ì¶”ë¡  ìµœì í™”
            with torch.no_grad():
                # JIT ì»´íŒŒì¼ (PyTorch 1.8+)
                try:
                    model = torch.jit.optimize_for_inference(model)
                    print("âœ… JIT ì¶”ë¡  ìµœì í™” ì™„ë£Œ")
                except:
                    print("âš ï¸ JIT ìµœì í™” ì‹¤íŒ¨ (PyTorch ë²„ì „ í™•ì¸ í•„ìš”)")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.clear_memory()
        
        return model
    
    def monitor_training_memory(self, model, dataloader, max_iterations: int = 10):
        """í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§"""
        print("ğŸ“ˆ í•™ìŠµ ì¤‘ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        model = model.to(self.device)
        model.train()
        
        memory_usage = []
        
        for i, batch in enumerate(dataloader):
            if i >= max_iterations:
                break
            
            try:
                # ë°°ì¹˜ë¥¼ GPUë¡œ ì´ë™
                if isinstance(batch, (list, tuple)):
                    batch = [item.to(self.device) if torch.is_tensor(item) else item for item in batch]
                else:
                    batch = batch.to(self.device)
                
                # ìˆœì „íŒŒ
                outputs = model(batch)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_usage.append(memory_used)
                
                print(f"   ë°˜ë³µ {i+1}: {memory_used:.2f}GB ì‚¬ìš©")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del outputs
                if i % 5 == 0:  # 5ë²ˆë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                    self.clear_memory()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"âŒ ë°˜ë³µ {i+1}ì—ì„œ OOM ì—ëŸ¬ ë°œìƒ")
                    break
                else:
                    raise e
        
        if memory_usage:
            avg_memory = sum(memory_usage) / len(memory_usage)
            max_memory = max(memory_usage)
            print(f"\nğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„:")
            print(f"   í‰ê· : {avg_memory:.2f}GB")
            print(f"   ìµœëŒ€: {max_memory:.2f}GB")
            print(f"   GPU ì‚¬ìš©ë¥ : {max_memory/self.gpu_memory_total*100:.1f}%")
        
        return memory_usage
    
    def get_recommended_settings(self) -> Dict:
        """RTX 3060Ti ê¶Œì¥ ì„¤ì • ë°˜í™˜"""
        settings = {
            'model_size': 'yolov8n',  # ê°€ì¥ ê°€ë²¼ìš´ ëª¨ë¸ ê¶Œì¥
            'batch_size': 8,
            'image_size': 640,
            'workers': 4,
            'mixed_precision': True,  # AMP ì‚¬ìš© ê¶Œì¥
            'gradient_accumulation': 2,  # ë°°ì¹˜ í¬ê¸° íš¨ê³¼ì  ì¦ê°€
            'memory_optimization': {
                'empty_cache_frequency': 10,  # 10 ì—í¬í¬ë§ˆë‹¤ ìºì‹œ ì •ë¦¬
                'gradient_checkpointing': True,
                'pin_memory': True
            }
        }
        
        print("ğŸ’¡ RTX 3060Ti ê¶Œì¥ ì„¤ì •:")
        for key, value in settings.items():
            print(f"   {key}: {value}")
        
        return settings


if __name__ == "__main__":
    # GPU ìµœì í™” í…ŒìŠ¤íŠ¸
    optimizer = GPUOptimizer()
    optimizer.print_memory_status()
    
    # ê¶Œì¥ ì„¤ì • ì¶œë ¥
    settings = optimizer.get_recommended_settings()
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    optimizer.clear_memory()
