"""
ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œ ì‹¤í–‰í•  ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
import time
import cv2
import numpy as np
import onnxruntime as ort
from pathlib import Path
import json


def load_onnx_model(model_path: str):
    """ONNX ëª¨ë¸ ë¡œë“œ"""
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    
    # CPUë§Œ ì‚¬ìš© (ë¼ì¦ˆë² ë¦¬íŒŒì´ëŠ” GPU ì—†ìŒ)
    providers = ['CPUExecutionProvider']
    
    session = ort.InferenceSession(
        model_path,
        sess_options=sess_options,
        providers=providers
    )
    
    input_name = session.get_inputs()[0].name
    input_shape = session.get_inputs()[0].shape
    input_size = (input_shape[2], input_shape[3])  # (width, height)
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"   ì…ë ¥ í¬ê¸°: {input_size}")
    
    return session, input_name, input_size


def preprocess_image(image_path: str, target_size: tuple) -> np.ndarray:
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    img = cv2.imread(str(image_path))
    if img is None:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    # ë¦¬ì‚¬ì´ì¦ˆ
    img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)
    
    # BGR -> RGB
    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
    
    # ì •ê·œí™” (0-255 -> 0-1)
    img_normalized = img_rgb.astype(np.float32) / 255.0
    
    # (H, W, C) -> (C, H, W)
    img_transposed = img_normalized.transpose(2, 0, 1)
    
    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, C, H, W)
    img_batch = np.expand_dims(img_transposed, axis=0)
    
    return img_batch


def postprocess_output(outputs: list, conf_threshold: float = 0.25) -> list:
    """YOLO ì¶œë ¥ í›„ì²˜ë¦¬"""
    detections = []
    
    if len(outputs) > 0:
        output = outputs[0]
        if output.ndim == 3:
            output = output[0]
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’ í•„í„°ë§
        confidences = output[:, 4]
        valid_indices = confidences > conf_threshold
        
        if np.any(valid_indices):
            valid_outputs = output[valid_indices]
            
            for det in valid_outputs:
                x_center, y_center, width, height, conf, class_id = det[:6]
                
                detections.append({
                    'bbox': [float(x_center), float(y_center), float(width), float(height)],
                    'confidence': float(conf),
                    'class_id': int(class_id)
                })
    
    return detections


def draw_results(image_path: str, detections: list, output_path: str = None):
    """ê²°ê³¼ ì‹œê°í™”"""
    img = cv2.imread(str(image_path))
    if img is None:
        return
    
    # í´ë˜ìŠ¤ ì´ë¦„ (ì‹¤ì œ í´ë˜ìŠ¤ì— ë§ê²Œ ìˆ˜ì •)
    class_names = [
        "29002", "34342", "37990", "39916", "40122", "40720", "40767", "40792",
        "40837", "40949", "40953", "40990", "40991", "41097", "41107", "41169",
        "41170", "41172", "41207", "41225", "41327", "41344"
    ]
    
    h, w = img.shape[:2]
    
    for det in detections:
        x_center, y_center, width, height = det['bbox']
        conf = det['confidence']
        class_id = det['class_id']
        
        # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        x1 = int((x_center - width/2) * w)
        y1 = int((y_center - height/2) * h)
        x2 = int((x_center + width/2) * w)
        y2 = int((y_center + height/2) * h)
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        label = f"{class_names[class_id] if class_id < len(class_names) else 'unknown'}: {conf:.2f}"
        cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    if output_path:
        cv2.imwrite(output_path, img)
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")
    else:
        cv2.imshow("Detection Result", img)
        cv2.waitKey(0)
        cv2.destroyAllWindows()


def main():
    parser = argparse.ArgumentParser(description="ë¼ì¦ˆë² ë¦¬íŒŒì´ ONNX ì¶”ë¡ ")
    parser.add_argument("--model", required=True, help="ONNX ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--image", required=True, help="ì…ë ¥ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--output", help="ì¶œë ¥ ì´ë¯¸ì§€ ê²½ë¡œ (ì„ íƒ)")
    parser.add_argument("--conf", type=float, default=0.25, help="ì‹ ë¢°ë„ ì„ê³„ê°’")
    parser.add_argument("--json", help="ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒ)")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë¡œë“œ
    session, input_name, input_size = load_onnx_model(args.model)
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    print(f"ğŸ“¸ ì´ë¯¸ì§€ ë¡œë“œ: {args.image}")
    img_batch = preprocess_image(args.image, input_size)
    
    # ì¶”ë¡ 
    print("ğŸ” ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    start_time = time.time()
    
    outputs = session.run(None, {input_name: img_batch})
    
    inference_time = time.time() - start_time
    
    # í›„ì²˜ë¦¬
    detections = postprocess_output(outputs, args.conf)
    
    print(f"âœ… ì¶”ë¡  ì™„ë£Œ")
    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time*1000:.1f}ms")
    print(f"   ê°ì§€ëœ ê°ì²´: {len(detections)}ê°œ")
    
    for i, det in enumerate(detections, 1):
        print(f"   {i}. í´ë˜ìŠ¤ {det['class_id']}: ì‹ ë¢°ë„ {det['confidence']:.2f}")
    
    # ê²°ê³¼ ì €ì¥
    if args.output:
        draw_results(args.image, detections, args.output)
    
    if args.json:
        result = {
            'image': args.image,
            'inference_time_ms': inference_time * 1000,
            'num_detections': len(detections),
            'detections': detections
        }
        with open(args.json, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"âœ… JSON ê²°ê³¼ ì €ì¥: {args.json}")


if __name__ == "__main__":
    main()

