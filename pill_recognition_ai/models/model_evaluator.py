"""
ëª¨ë¸ í‰ê°€ ë° ì„±ëŠ¥ ì¸¡ì • ëª¨ë“ˆ
mAP, precision, recall, FPS ë“±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import os
import time
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import cv2
import torch
from ultralytics import YOLO
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_recall_curve, average_precision_score
import yaml
from tqdm import tqdm


class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "configs/training_config.yaml"):
        """ì´ˆê¸°í™”"""
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.eval_config = self.config['evaluation']
        self.inference_config = self.config['inference']
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
    
    def load_model(self, model_path: str) -> YOLO:
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ: {model_path}")
        model = YOLO(model_path)
        return model
    
    def evaluate_model(self, model_path: str, dataset_yaml_path: str) -> Dict:
        """ëª¨ë¸ ì¢…í•© í‰ê°€"""
        print("ğŸ” ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        # ëª¨ë¸ ë¡œë“œ
        model = self.load_model(model_path)
        
        # ê¸°ë³¸ ê²€ì¦ ì‹¤í–‰
        val_results = model.val(
            data=dataset_yaml_path,
            imgsz=self.config['data']['image_size'],
            conf=self.eval_config['confidence_threshold'],
            iou=0.45,  # ê¸°ë³¸ NMS ì„ê³„ê°’
            verbose=True
        )
        
        # í‰ê°€ ê²°ê³¼ ì •ë¦¬
        evaluation_results = {
            'model_path': model_path,
            'dataset_path': dataset_yaml_path,
            'basic_metrics': {
                'mAP@0.5': float(val_results.box.map50),
                'mAP@0.5:0.95': float(val_results.box.map),
                'precision': float(val_results.box.mp),
                'recall': float(val_results.box.mr),
                'f1_score': 2 * (val_results.box.mp * val_results.box.mr) / (val_results.box.mp + val_results.box.mr) if (val_results.box.mp + val_results.box.mr) > 0 else 0
            },
            'detailed_metrics': self._extract_detailed_metrics(val_results),
            'class_metrics': self._extract_class_metrics(val_results),
            'inference_speed': self._measure_inference_speed(model, dataset_yaml_path)
        }
        
        # ê²°ê³¼ ì €ì¥
        self._save_evaluation_results(evaluation_results)
        
        # ì‹œê°í™” ìƒì„±
        self._create_evaluation_plots(evaluation_results, val_results)
        
        print("âœ… ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
        return evaluation_results
    
    def _extract_detailed_metrics(self, val_results) -> Dict:
        """ìƒì„¸ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        detailed_metrics = {}
        
        # IoUë³„ mAP ê³„ì‚°
        if hasattr(val_results.box, 'maps'):
            detailed_metrics['mAP_by_iou'] = {
                f'IoU_{iou:.2f}': float(map_val) 
                for iou, map_val in zip(self.eval_config['iou_thresholds'], val_results.box.maps)
            }
        
        # í´ë˜ìŠ¤ë³„ ìƒì„¸ ë©”íŠ¸ë¦­
        if hasattr(val_results.box, 'ap_class_index'):
            detailed_metrics['per_class_ap'] = {
                str(idx): float(ap) for idx, ap in zip(val_results.box.ap_class_index, val_results.box.ap)
            }
        
        return detailed_metrics
    
    def _extract_class_metrics(self, val_results) -> Dict:
        """í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        class_metrics = {}
        
        if hasattr(val_results.box, 'ap_class_index') and hasattr(val_results.box, 'ap'):
            for idx, ap in zip(val_results.box.ap_class_index, val_results.box.ap):
                class_name = self.config['data']['classes'][idx] if idx < len(self.config['data']['classes']) else f"class_{idx}"
                class_metrics[class_name] = {
                    'ap@0.5': float(ap),
                    'ap@0.5:0.95': float(val_results.box.map)  # ì „ì²´ í‰ê·  ì‚¬ìš©
                }
        
        return class_metrics
    
    def _measure_inference_speed(self, model: YOLO, dataset_yaml_path: str) -> Dict:
        """ì¶”ë¡  ì†ë„ ì¸¡ì •"""
        print("â±ï¸ ì¶”ë¡  ì†ë„ ì¸¡ì • ì¤‘...")
        
        try:
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë¡œë“œ
            test_images_dir = Path(dataset_yaml_path).parent / "test" / "images"
            if not test_images_dir.exists():
                print("âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            test_images = list(test_images_dir.glob("*.png"))[:10]  # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
            
            if not test_images:
                print("âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            # ê°„ë‹¨í•œ ì†ë„ ì¸¡ì • (ì˜¤ë¥˜ ë°©ì§€)
            speed_results = {
                'cpu_fps': 0.0,
                'cpu_latency_ms': 0.0,
                'test_images': len(test_images),
                'note': 'Speed measurement skipped due to PyTorch compatibility issue'
            }
            
            print("âš ï¸ ì¶”ë¡  ì†ë„ ì¸¡ì • ê±´ë„ˆë›°ê¸° (PyTorch í˜¸í™˜ì„± ë¬¸ì œ)")
            return speed_results
            
        except Exception as e:
            print(f"âš ï¸ ì¶”ë¡  ì†ë„ ì¸¡ì • ì‹¤íŒ¨: {e}")
            return {
                'cpu_fps': 0.0,
                'cpu_latency_ms': 0.0,
                'test_images': 0,
                'error': str(e)
            }
    
    def _save_evaluation_results(self, results: Dict):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = self.results_dir / f"evaluation_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {results_file}")
    
    def _create_evaluation_plots(self, results: Dict, val_results):
        """í‰ê°€ ê²°ê³¼ ì‹œê°í™”"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        plots_dir = self.results_dir / f"evaluation_plots_{timestamp}"
        plots_dir.mkdir(exist_ok=True)
        
        # 1. ê¸°ë³¸ ë©”íŠ¸ë¦­ ë§‰ëŒ€ ê·¸ë˜í”„
        self._plot_basic_metrics(results['basic_metrics'], plots_dir)
        
        # 2. í´ë˜ìŠ¤ë³„ AP íˆíŠ¸ë§µ
        if results['class_metrics']:
            self._plot_class_metrics(results['class_metrics'], plots_dir)
        
        # 3. ì¶”ë¡  ì†ë„ ë¹„êµ
        if results['inference_speed']:
            self._plot_inference_speed(results['inference_speed'], plots_dir)
        
        print(f"ğŸ“Š ì‹œê°í™” ê²°ê³¼ ì €ì¥: {plots_dir}")
    
    def _plot_basic_metrics(self, metrics: Dict, plots_dir: Path):
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ì‹œê°í™”"""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        metric_names = list(metrics.keys())
        metric_values = list(metrics.values())
        
        bars = ax.bar(metric_names, metric_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom')
        
        ax.set_ylabel('Score')
        ax.set_title('Model Performance Metrics')
        ax.set_ylim(0, 1.1)
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(plots_dir / 'basic_metrics.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_class_metrics(self, class_metrics: Dict, plots_dir: Path):
        """í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­ ì‹œê°í™”"""
        if not class_metrics:
            return
        
        # í´ë˜ìŠ¤ë³„ AP ë°ì´í„° ì¤€ë¹„
        classes = list(class_metrics.keys())
        ap_values = [class_metrics[cls]['ap@0.5'] for cls in classes]
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        bars = ax.bar(range(len(classes)), ap_values, color='skyblue')
        
        # ê°’ í‘œì‹œ
        for i, (bar, value) in enumerate(zip(bars, ap_values)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{value:.3f}', ha='center', va='bottom')
        
        ax.set_xlabel('Pill Classes')
        ax.set_ylabel('Average Precision (AP@0.5)')
        ax.set_title('Average Precision by Pill Class')
        ax.set_xticks(range(len(classes)))
        ax.set_xticklabels(classes, rotation=45, ha='right')
        ax.set_ylim(0, 1.1)
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'class_metrics.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_inference_speed(self, speed_metrics: Dict, plots_dir: Path):
        """ì¶”ë¡  ì†ë„ ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # FPS ë¹„êµ
        devices = ['CPU']
        fps_values = [speed_metrics['cpu_fps']]
        
        if 'gpu_fps' in speed_metrics:
            devices.append('GPU')
            fps_values.append(speed_metrics['gpu_fps'])
        
        bars1 = ax1.bar(devices, fps_values, color=['#ff7f0e', '#2ca02c'])
        ax1.set_ylabel('FPS')
        ax1.set_title('Inference Speed (FPS)')
        
        for bar, value in zip(bars1, fps_values):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{value:.1f}', ha='center', va='bottom')
        
        # ì§€ì—°ì‹œê°„ ë¹„êµ
        latency_values = [speed_metrics['cpu_latency_ms']]
        if 'gpu_latency_ms' in speed_metrics:
            latency_values.append(speed_metrics['gpu_latency_ms'])
        
        bars2 = ax2.bar(devices, latency_values, color=['#ff7f0e', '#2ca02c'])
        ax2.set_ylabel('Latency (ms)')
        ax2.set_title('Inference Latency')
        
        for bar, value in zip(bars2, latency_values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{value:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(plots_dir / 'inference_speed.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def compare_models(self, model_paths: List[str], dataset_yaml_path: str) -> pd.DataFrame:
        """ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        print("ğŸ”„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘...")
        
        comparison_results = []
        
        for model_path in model_paths:
            print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€: {model_path}")
            results = self.evaluate_model(model_path, dataset_yaml_path)
            
            comparison_results.append({
                'model': Path(model_path).stem,
                'mAP@0.5': results['basic_metrics']['mAP@0.5'],
                'mAP@0.5:0.95': results['basic_metrics']['mAP@0.5:0.95'],
                'precision': results['basic_metrics']['precision'],
                'recall': results['basic_metrics']['recall'],
                'f1_score': results['basic_metrics']['f1_score'],
                'cpu_fps': results['inference_speed'].get('cpu_fps', 0),
                'gpu_fps': results['inference_speed'].get('gpu_fps', 0)
            })
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(comparison_results)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        comparison_file = self.results_dir / f"model_comparison_{timestamp}.csv"
        df.to_csv(comparison_file, index=False)
        
        print(f"ğŸ“ˆ ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_file}")
        print("\nğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        print(df.to_string(index=False))
        
        return df


if __name__ == "__main__":
    # ëª¨ë¸ í‰ê°€ ì‹¤í–‰
    evaluator = ModelEvaluator()
    
    # í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ
    model_path = "results/training_20241027_085100/best_model.pt"
    dataset_yaml = "dataset/pill_dataset.yaml"
    
    # ë‹¨ì¼ ëª¨ë¸ í‰ê°€
    if os.path.exists(model_path):
        results = evaluator.evaluate_model(model_path, dataset_yaml)
        print("\nğŸ“Š í‰ê°€ ê²°ê³¼:")
        for metric, value in results['basic_metrics'].items():
            print(f"   {metric}: {value:.4f}")
    else:
        print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
    
    # ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ (ì˜ˆì‹œ)
    # model_paths = [
    #     "results/training_20241027_085100/best_model.pt",
    #     "results/training_20241027_090000/best_model.pt"
    # ]
    # evaluator.compare_models(model_paths, dataset_yaml)
